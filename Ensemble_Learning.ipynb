{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning\n",
        "\n",
        "1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "- Ensemble Learning is a machine learning technique in which multiple models (called base learners) are trained and combined to solve the same problem. Instead of relying on a single model, ensemble learning combines the predictions of several models to produce a more accurate and reliable result.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of weak or moderately accurate models, when combined, can perform better than any individual model. This works because different models make different errors, and combining them helps reduce overall error, variance, and sometimes bias.\n",
        "\n",
        "2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "- Bagging and Boosting are two popular ensemble learning techniques used to improve model performance by combining multiple models.\n",
        "\n",
        "Bagging (Bootstrap Aggregating) works by training several models on different random samples of the training data. Each model is trained independently using bootstrap sampling, and the final prediction is made by averaging or voting. Bagging mainly helps in reducing variance and prevents overfitting. Random Forest is a common example of Bagging.\n",
        "\n",
        "Boosting, on the other hand, trains models sequentially. Each new model focuses more on the data points that were misclassified by previous models. Boosting assigns higher weights to incorrectly predicted samples so that the next model learns from those mistakes. This method helps reduce bias and improve overall accuracy. Examples of Boosting include AdaBoost and Gradient Boosting.\n",
        "\n",
        "In summary, Bagging builds independent models to reduce variance, while Boosting builds dependent models to improve accuracy by learning from errors.\n",
        "\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "- Bootstrap sampling is a technique where multiple datasets are created from the original dataset by randomly sampling with replacement. This means some data points may appear multiple times, while others may not appear at all in a sample.\n",
        "\n",
        "In Bagging methods like Random Forest:\n",
        "\n",
        "Each decision tree is trained on a different bootstrap sample.\n",
        "\n",
        "This creates diversity among trees.\n",
        "\n",
        "Diversity helps reduce overfitting and improves generalization.\n",
        "\n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "- Out-of-Bag (OOB) samples are the data points that are not selected in a bootstrap sample for training a particular model.\n",
        "\n",
        "In Random Forest:\n",
        "\n",
        "- About 36% of data is left out for each tree.\n",
        "\n",
        "- These OOB samples are used to test the model.\n",
        "\n",
        "- The OOB score is calculated by averaging predictions on OOB samples.\n",
        "\n",
        "OOB score provides an unbiased estimate of model performance without needing a separate validation dataset.\n",
        "\n",
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "- Single Decision Tree:\n",
        "\n",
        "Feature importance is based on how much a feature reduces impurity.\n",
        "\n",
        "Results can be unstable and sensitive to noise.\n",
        "\n",
        "May overfit the data.\n",
        "\n",
        "- Random Forest:\n",
        "\n",
        "Feature importance is averaged across many trees.\n",
        "\n",
        "More stable and reliable.\n",
        "\n",
        "Less sensitive to noise and overfitting."
      ],
      "metadata": {
        "id": "BfpH9R_rZlkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.Write a Python program to:\n",
        "#● Load the Breast Cancer dataset using\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importance = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "top5 = importance.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXuz_XwkhOJN",
        "outputId": "c1e31a6f-ab7d-4017-c71e-2f27409d6044"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.  Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgZmIVXphrTW",
        "outputId": "64f2fad7-a30e-41aa-a549-aca60fd30771"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFfM1VqAh8pO",
        "outputId": "30e0e57a-cfeb-4643-c232-6e10ec57cd2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Best Accuracy: 0.9560937742586555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.  Write a Python program to:\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the CaliforniaHousing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "bag = BaggingRegressor(random_state=42)\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "print(\"Bagging MSE:\", bag_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBvwGQURiNab",
        "outputId": "cf2cf5ed-b45a-4bee-b659-9f20fb8b4cfd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.27872374841230696\n",
            "Random Forest MSE: 0.2542358390056568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Ensemble Learning for Loan Default Prediction\n",
        "\n",
        "- Choosing Bagging or Boosting:\n",
        "\n",
        "Use Boosting (like Gradient Boosting) to improve prediction accuracy.\n",
        "\n",
        "If data is noisy, prefer Bagging.\n",
        "\n",
        "- Handling Overfitting:\n",
        "\n",
        "Use cross-validation.\n",
        "\n",
        "Limit tree depth.\n",
        "\n",
        "- Use regularization parameters.\n",
        "\n",
        "Selecting Base Models:\n",
        "\n",
        "Decision Trees are preferred as base learners.\n",
        "\n",
        "They capture non-linear relationships.\n",
        "\n",
        "- Evaluating Performance:\n",
        "\n",
        "Use k-fold cross-validation.\n",
        "\n",
        "Evaluate using accuracy, ROC-AUC, precision, and recall.\n",
        "\n",
        "- Justification:\n",
        "Ensemble learning improves decision-making by:\n",
        "\n",
        "Reducing risk of wrong predictions\n",
        "\n",
        "Improving accuracy\n",
        "\n",
        "Making the model more stable and reliable"
      ],
      "metadata": {
        "id": "vTFdBwMcilxB"
      }
    }
  ]
}